import tensorflow as tf
import numpy as np
from instances import get_instances
from utils import write_w2v_format


def train(feeder, output_file, dim=100, epochs=1000, batch_size=128, verbose=1, init_embeddings=None):
    """    Initializes and trains the model. Number of positive and negative inputs are dynamically created based on positive
    and negative sampling. Model is trained for multiple epochs and batch data is generated by the feeder object. Write
    learned embeddings to file.

    Args:
        feeder: holds information about the used corpus. Generated batch data
        output_file: write the learned embeddings to this file
        dim: dimension of the embedding
        epochs: number of training epochs
        batch_size: size of one batch
        verbose: level of verbosity
        init_embeddings: weights to initialize embeddings for additional training

    """
    print('train model...')
    vocab_len = len(feeder.vocab)
    neg_sampling = feeder.neg_sampling
    pos_sampling = feeder.pos_sampling
    init_string = 'NoInit'

    # create initial embedding weights, sorted by vocabulary indices from the feeder
    if init_embeddings is not None:
        init_string = 'Init'
        sorted_embeddings = list(None for _ in range(len(feeder.vocab)))
        for word, idx in feeder.vocab.items():
            sorted_embeddings[idx] = init_embeddings[word]
        sorted_embeddings = np.asarray(sorted_embeddings)

    output_file = output_file.format(dim=dim, epochs=epochs, neg_sampling=neg_sampling, pos_sampling=pos_sampling,
                                     init=init_string)

    graph = tf.Graph()
    with graph.as_default():

        if init_embeddings is not None:
            embedding_weights = tf.Variable(initial_value=sorted_embeddings, name='embeddings')
        else:
            # initialize embeddings by drawing from a normal distribution; Paragraph 3.4
            embedding_weights = tf.Variable(tf.random_normal([vocab_len, dim], mean=0.0, stddev=0.01),
                                            name='embeddings')

        # create labels, same for every instance; 1/pos_sampling for positive pairs; 0 for negative_pairs; sum = 1
        labels = np.zeros(shape=(batch_size, pos_sampling + neg_sampling))
        labels[:, range(pos_sampling)] = 1.0 / pos_sampling
        labels = tf.constant(labels, dtype=tf.float32)

        # input sentence i
        base_input = tf.sparse_placeholder(dtype=tf.int64)
        # average layer; get embedding for every word in the sequence and calculate the mean vector for them
        base_embedding = tf.nn.embedding_lookup_sparse(embedding_weights, base_input, None, name=None,
                                                       combiner='mean')

        # dynamically create positive inputs
        pos_embeddings = list()
        pos_inputs = list()
        for i in range(pos_sampling):
            pos_input = tf.sparse_placeholder(dtype=tf.int64)
            pos_inputs.append(pos_input)
            pos_embedding = tf.nn.embedding_lookup_sparse(embedding_weights, pos_input, None, name=None,
                                                          combiner='mean')
            pos_embeddings.append(pos_embedding)

        # dynamically create negative inputs
        neg_embeddings = list()
        neg_inputs = list()
        for i in range(neg_sampling):
            neg_input = tf.sparse_placeholder(dtype=tf.int64)
            neg_inputs.append(neg_input)
            neg_embedding = tf.nn.embedding_lookup_sparse(embedding_weights, neg_input, None, name=None,
                                                          combiner='mean')
            neg_embeddings.append(neg_embedding)

        # cosine layer
        cos_similarities = cosine_similarity(base_embedding, pos_embeddings + neg_embeddings)

        # softmax on the categorical cross entropy between cosine similarities and labels
        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=cos_similarities)
        loss_batch = tf.reduce_mean(loss)

        # set learning rate as variable to be able to manipulate it every batch
        learning_rate = tf.placeholder(tf.float32, shape=[])
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

    with tf.Session(graph=graph) as session:
        tf.global_variables_initializer().run()

        for epoch in range(epochs):
            if verbose > 0:
                print('epoch: {}/{}'.format(epoch+1, epochs))
            loss_batches = list()
            batch = 0

            # get and train batches per epoch till generator is exhausted
            for base_batch, pos_batches, neg_batches in get_instances(feeder, batch_size):

                # feed current batch
                feed_dict = {base_input: base_batch}
                feed_dict.update({pos_inputs[i]: pos_batches[i] for i in range(pos_sampling)})
                feed_dict.update({neg_inputs[i]: neg_batches[i] for i in range(neg_sampling)})

                # initialize learning rate with 0.0001; Paragraph 3.4
                # learning rate monotonically decreases proportionally to the number of batches; Paragraph 2.3
                feed_dict.update({learning_rate: 0.0001 / (batch+1)})
                batch += 1

                # train model
                _, loss_batch_val = session.run([optimizer, loss_batch], feed_dict=feed_dict)
                loss_batches.append(loss_batch_val)

            # print status messages
            if verbose > 0 and epoch % 1 == 0:
                print('Average loss epoch {}: {}'.format(epoch+1, sum(loss_batches) / len(loss_batches)))

        # save embeddings
        embeddings = session.run(embedding_weights)
        write_w2v_format(embeddings, feeder.vocab, output_file)


def normalize_embeddings(embeddings):
    """
    Calculate norm for the given embeddings.
    """
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    return normalized_embeddings


def cosine_similarity(base_embedding, compare_embeddings):
    """Calculate cosine similarity between the base embedding and every compare embedding (positive and negative partners).

    Args:
        base_embedding:
        compare_embeddings: list of positive and negative partner embeddings

    Returns:tensor of cosine similarities of shape (batch_size, number of partners)

    """
    normalized_base_emb = normalize_embeddings(base_embedding)
    cos_similarities = list()
    for compare_embedding in compare_embeddings:
        normalized_compare_emb = normalize_embeddings(compare_embedding)
        cos_sim = tf.reduce_sum(tf.multiply(normalized_base_emb, normalized_compare_emb), axis=1, keep_dims=True)
        cos_similarities.append(cos_sim)
    cos_similarities = tf.concat(cos_similarities, axis=1)
    return cos_similarities
